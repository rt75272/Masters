import numpy as np
import tensorflow as tf
from data_generator import DataGenerator
from encoder_decoder_model import EncoderDecoderModel
from model_trainer import ModelTrainer
from gpu_config import configure_gpu
"""Keras Encoder-Decoder Sequence-to-Sequence Model.

Main entry point for the encoder-decoder sequence-to-sequence model.
Coordinates data generation, model training, and evaluation.

Usage:
    $ python main.py
"""
NUM_COLUMNS = 88

def display_neural_network_flowchart():
    """Display a comprehensive flowchart of the neural network processing pipeline."""
    print("\n" + "=" * NUM_COLUMNS)
    print("ğŸ§  NEURAL NETWORK PROCESSING FLOWCHART")
    print("=" * NUM_COLUMNS)
    flowchart = """
    ğŸ“Š DATA GENERATION PHASE
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ² Random Sequence Generator                                                   â”‚
    â”‚  â”œâ”€ Generate random integers [0, cardinality-1]                                 â”‚
    â”‚  â”œâ”€ Input sequences: 6 timesteps                                                â”‚
    â”‚  â”œâ”€ Output sequences: 3 timesteps                                               â”‚
    â”‚  â””â”€ One-hot encode: [batch, timesteps, features]                                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
    ğŸ”„ PREPROCESSING PHASE
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ“ Data Reshaping & Preparation                                                â”‚
    â”‚  â”œâ”€ X1 (Encoder Input): [15000, 6, 51] - Input sequences                        â”‚
    â”‚  â”œâ”€ X2 (Decoder Input): [15000, 3, 51] - Target sequences (shifted)             â”‚
    â”‚  â””â”€ Training/Validation split: 80/20                                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
    ğŸ§  ENCODER-DECODER ARCHITECTURE
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ—ï¸  ENCODER BRANCH                                                             â”‚
    â”‚  â”œâ”€ Input: X1 [batch, 6, 51]                                                    â”‚
    â”‚  â”œâ”€ LSTM Layer: 256 units, return_state=True                                    â”‚
    â”‚  â”œâ”€ Dropout: 20% (regularization)                                               â”‚
    â”‚  â”œâ”€ Batch Normalization                                                         â”‚
    â”‚  â””â”€ Output: Hidden states (h, c) â†’ passed to decoder                            â”‚
    â”‚                                                                                 â”‚
    â”‚  ğŸ—ï¸  DECODER BRANCH                                                             â”‚
    â”‚  â”œâ”€ Input: X2 [batch, 3, 51] + Encoder states                                   â”‚
    â”‚  â”œâ”€ LSTM Layer: 256 units, return_sequences=True                                â”‚
    â”‚  â”œâ”€ Initial state: from encoder (h, c)                                          â”‚
    â”‚  â”œâ”€ Dropout: 20% (regularization)                                               â”‚
    â”‚  â”œâ”€ Batch Normalization                                                         â”‚
    â”‚  â”œâ”€ Dense Layer: 51 units (vocabulary size)                                     â”‚
    â”‚  â”œâ”€ Activation: Softmax (probability distribution)                              â”‚
    â”‚  â””â”€ Output: [batch, 3, 51] - Predicted sequences                                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
    âš™ï¸  TRAINING PHASE
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ¯ Loss Function & Optimization                                                â”‚
    â”‚  â”œâ”€ Loss: Categorical Crossentropy                                              â”‚
    â”‚  â”œâ”€ Optimizer: Adam (learning_rate=0.001)                                       â”‚
    â”‚  â”œâ”€ Metrics: Accuracy                                                           â”‚
    â”‚  â”œâ”€ Epochs: 100                                                                 â”‚
    â”‚  â”œâ”€ Batch Size: 64                                                              â”‚
    â”‚  â””â”€ Callbacks: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
    ğŸ“ˆ EVALUATION PHASE
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ” Model Assessment                                                            â”‚
    â”‚  â”œâ”€ Generate test sequences                                                     â”‚
    â”‚  â”œâ”€ Forward pass through trained model                                          â”‚
    â”‚  â”œâ”€ Decode predictions to integers                                              â”‚
    â”‚  â”œâ”€ Compare with ground truth                                                   â”‚
    â”‚  â”œâ”€ Calculate accuracy metrics                                                  â”‚
    â”‚  â””â”€ Generate sample predictions for inspection                                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
    ğŸ’¾ OUTPUT GENERATION
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ“„ Results & Predictions                                                       â”‚
    â”‚  â”œâ”€ Formatted accuracy report                                                   â”‚
    â”‚  â”œâ”€ Sample input/output comparisons                                             â”‚
    â”‚  â”œâ”€ Model performance metrics                                                   â”‚
    â”‚  â””â”€ Saved predictions to file                                                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ”¬ TECHNICAL DETAILS:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Architecture: Encoder-Decoder with Teacher Forcing                             â”‚
    â”‚  Task Type: Sequence-to-Sequence (Many-to-Many)                                 â”‚
    â”‚  Attention: Implicit through state transfer                                     â”‚
    â”‚  Regularization: Dropout + Batch Normalization                                  â”‚
    â”‚  Hardware: GPU-accelerated (CUDA) with CPU fallback                             â”‚
    â”‚  Memory: Dynamic GPU memory growth                                              â”‚
    â”‚  Reproducibility: Fixed random seeds (42)                                       â”‚   
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print(flowchart)
    print("=" * NUM_COLUMNS)

def main():
    """Main driver function to run the encoder-decoder sequence-to-sequence model."""
    # Configure GPU before any other operations.
    gpu_available = configure_gpu()
    # Set random seed for reproducibility.
    np.random.seed(42)
    tf.random.set_seed(42)
    # Display system information.
    print("=" * NUM_COLUMNS)
    print("ğŸš€ ENCODER-DECODER SEQUENCE-TO-SEQUENCE MODEL")
    print("=" * NUM_COLUMNS)
    print("ğŸ“‹ System Configuration:")
    print(f" ğŸ”§ TensorFlow version: {tf.__version__}")
    print(f" ğŸ–¥ï¸  GPU Available: {len(tf.config.list_physical_devices('GPU'))} device(s)")
    print(f" ğŸ”— Built with CUDA: {tf.test.is_built_with_cuda()}")
    if tf.config.list_physical_devices('GPU'):
        gpu_name = str(tf.config.list_physical_devices('GPU')[0]).split("name='")[1].split("'")[0]
        print(f"   ğŸ® GPU Device: {gpu_name}")
    device_type = "GPU" if gpu_available else "CPU"
    print(f"   âš¡ Execution Device: {device_type}")
    print("=" * NUM_COLUMNS)
    n_features = 50 + 1
    n_steps_in = 6
    n_steps_out = 3
    n_units = 256
    n_samples = 15000 
    dropout_rate = 0.2 # Regularization.
    print("\n" + "=" * NUM_COLUMNS)
    print("ğŸ“Š MODEL CONFIGURATION")
    print("=" * NUM_COLUMNS)
    print(f"   ğŸ”¢ Vocabulary size: {n_features} classes")
    print(f"   ğŸ“ Input sequence length: {n_steps_in}")
    print(f"   ğŸ“ Output sequence length: {n_steps_out}")
    print(f"   ğŸ§  LSTM units: {n_units}")
    print(f"   ğŸ¯ Training samples: {n_samples:,}")
    print(f"   ğŸ›¡ï¸  Dropout rate: {dropout_rate}")
    print("=" * NUM_COLUMNS)
    # Display comprehensive neural network flowchart.
    display_neural_network_flowchart()
    # Initialize data generator, model, and trainer.
    data_generator = DataGenerator(cardinality=n_features, random_seed=42)
    model = EncoderDecoderModel(n_features, n_features, n_units, 
                               use_gpu=gpu_available, dropout_rate=dropout_rate)
    trainer = ModelTrainer(model, data_generator)
    print("\nğŸ“ˆ GENERATING DATASET...")
    X1, X2, y = data_generator.get_dataset(n_steps_in, 
                                        n_steps_out, 
                                        n_samples)
    X1 = X1.reshape((n_samples, n_steps_in, n_features))
    X2 = X2.reshape((n_samples, n_steps_out, n_features))
    y = y.reshape((n_samples, n_steps_out, n_features))
    print("ğŸ”§ COMPILING MODEL...")
    model.compile_model(optimizer='adam', 
                       loss='categorical_crossentropy', 
                       metrics=['accuracy'],
                       learning_rate=0.001)
    print(f"\nğŸ‹ï¸  TRAINING MODEL ON {device_type}...")
    print("-" * NUM_COLUMNS)
    model.train(X1, X2, y, epochs=100, batch_size=64, verbose=2, use_callbacks=True)
    # Evaluate and save predictions.
    trainer.evaluate_accuracy(n_steps_in, n_steps_out, n_features, n_test_samples=100)
    trainer.save_predictions(n_steps_in, n_steps_out, n_features, 
                           filename='output_predictions.txt', n_samples=10)
    print("\n" + "=" * NUM_COLUMNS)
    print("ğŸ‰ TRAINING AND EVALUATION COMPLETE!")
    print("=" * NUM_COLUMNS)
    if not gpu_available:
        print("\nâš ï¸  Note: Training completed on CPU.")
        print("   For GPU acceleration, ensure CUDA drivers and libraries are installed.")
    else:
        print("=" * NUM_COLUMNS)
        print(f"\nâœ… Successfully utilized {device_type} for training.")
    print("=" * NUM_COLUMNS)

# The big red activation button.
if __name__ == '__main__':
    main()
