import argparse
from datetime import datetime
"""Neural Network Architecture Visualizer.

This script generates comprehensive visual representations of the 
encoder-decoder sequence-to-sequence model architecture and data flow.

Usage:
    python visualize_architecture.py
    python visualize_architecture.py --detailed    # More technical details
    python visualize_architecture.py --save        # Save to file
"""

def generate_detailed_architecture_diagram():
    """Generate a detailed technical architecture diagram."""
    return """
ğŸ§  DETAILED ENCODER-DECODER ARCHITECTURE DIAGRAM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INPUT DATA FLOW:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           BATCH PROCESSING                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Batch Size: 64 samples                                                         â”‚
â”‚  Input Shape: [64, 6, 51]  â”‚  Decoder Input: [64, 3, 51]                        â”‚
â”‚  Output Shape: [64, 3, 51] â”‚  Expected Output: [64, 3, 51]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             ENCODER SECTION                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  ğŸ“Š Input Layer                                                                 â”‚
â”‚  â”œâ”€ Shape: [batch=64, timesteps=6, features=51]                                 â”‚
â”‚  â”œâ”€ Data Type: float32                                                          â”‚
â”‚  â””â”€ Encoding: One-hot vectors for integer sequences                             â”‚
â”‚                                      â”‚                                          â”‚
â”‚                                      â–¼                                          â”‚
â”‚  ğŸ§  LSTM Layer (Encoder)                                                        â”‚
â”‚  â”œâ”€ Units: 256                                                                  â”‚
â”‚  â”œâ”€ Return State: True                                                          â”‚
â”‚  â”œâ”€ Return Sequences: False                                                     â”‚
â”‚  â”œâ”€ Activation: tanh (default)                                                  â”‚
â”‚  â”œâ”€ Recurrent Activation: sigmoid (default)                                     â”‚
â”‚  â””â”€ Parameters: ~263,168 trainable params                                       â”‚
â”‚                                      â”‚                                          â”‚
â”‚                                      â–¼                                          â”‚
â”‚  ğŸ›¡ï¸  Dropout Layer                                                              â”‚
â”‚  â”œâ”€ Rate: 0.2 (20% neurons dropped during training)                             â”‚
â”‚  â””â”€ Purpose: Prevent overfitting                                                â”‚
â”‚                                      â”‚                                          â”‚
â”‚                                      â–¼                                          â”‚
â”‚  âš¡ Batch Normalization                                                         â”‚
â”‚  â”œâ”€ Normalize activations                                                       â”‚
â”‚  â”œâ”€ Accelerate training                                                         â”‚
â”‚  â””â”€ Improve gradient flow                                                       â”‚
â”‚                                      â”‚                                          â”‚
â”‚                                      â–¼                                          â”‚
â”‚  ğŸ“¤ State Output                                                                â”‚
â”‚  â”œâ”€ Hidden State (h): [batch=64, units=256]                                     â”‚
â”‚  â”œâ”€ Cell State (c): [batch=64, units=256]                                       â”‚
â”‚  â””â”€ These states encode the entire input sequence                               â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             DECODER SECTION                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  ğŸ“Š Input Layer                                                                 â”‚
â”‚  â”œâ”€ Shape: [batch=64, timesteps=3, features=51]                                 â”‚
â”‚  â”œâ”€ Teacher Forcing: Uses ground truth during training                          â”‚
â”‚  â””â”€ Initial States: From encoder (h, c)                                         â”‚
â”‚                                      â”‚                                          â”‚
â”‚                                      â–¼                                          â”‚
â”‚  ğŸ§  LSTM Layer (Decoder)                                                        â”‚
â”‚  â”œâ”€ Units: 256                                                                  â”‚
â”‚  â”œâ”€ Return Sequences: True                                                      â”‚
â”‚  â”œâ”€ Return State: False                                                         â”‚
â”‚  â”œâ”€ Initial State: [encoder_h, encoder_c]                                       â”‚
â”‚  â””â”€ Output Shape: [batch=64, timesteps=3, units=256]                            â”‚
â”‚                                      â”‚                                          â”‚
â”‚                                      â–¼                                          â”‚
â”‚  ğŸ›¡ï¸  Dropout Layer                                                              â”‚
â”‚  â”œâ”€ Rate: 0.2 (20% neurons dropped during training)                             â”‚
â”‚  â””â”€ Applied to each timestep                                                    â”‚
â”‚                                      â”‚                                          â”‚
â”‚                                      â–¼                                          â”‚
â”‚  âš¡ Batch Normalization                                                         â”‚
â”‚  â”œâ”€ Applied across time dimension                                               â”‚
â”‚  â””â”€ Stabilizes training dynamics                                                â”‚
â”‚                                      â”‚                                          â”‚
â”‚                                      â–¼                                          â”‚
â”‚  ğŸ¯ Dense Output Layer                                                          â”‚
â”‚  â”œâ”€ Units: 51 (vocabulary size)                                                 â”‚
â”‚  â”œâ”€ Activation: Softmax                                                         â”‚
â”‚  â”œâ”€ Output Shape: [batch=64, timesteps=3, classes=51]                           â”‚
â”‚  â””â”€ Produces probability distribution over vocabulary                           â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           LOSS & OPTIMIZATION                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  ğŸ“Š Loss Function: Categorical Crossentropy                                     â”‚
â”‚  â”œâ”€ Formula: -Î£(y_true * log(y_pred))                                           â”‚
â”‚  â”œâ”€ Applied per timestep, averaged across batch                                 â”‚
â”‚  â””â”€ Suitable for multi-class classification                                     â”‚
â”‚                                                                                 â”‚
â”‚  âš™ï¸  Optimizer: Adam                                                            â”‚
â”‚  â”œâ”€ Learning Rate: 0.001                                                        â”‚
â”‚  â”œâ”€ Beta1: 0.9, Beta2: 0.999                                                    â”‚
â”‚  â”œâ”€ Epsilon: 1e-07                                                              â”‚
â”‚  â””â”€ Adaptive learning rate per parameter                                        â”‚
â”‚                                                                                 â”‚
â”‚  ğŸ“ˆ Metrics: Accuracy                                                           â”‚
â”‚  â”œâ”€ Measures exact sequence match                                               â”‚
â”‚  â””â”€ Calculated per timestep and averaged                                        â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TRAINING DYNAMICS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”„ FORWARD PASS:
   1. Encoder processes input sequence [1,2,3,4,5,6]
   2. Encoder states (h,c) capture sequence information
   3. Decoder receives target sequence [4,5,6] + encoder states
   4. Decoder outputs probability distributions for [5,6,7]
   5. Loss computed against true targets [5,6,7]

â¬…ï¸  BACKWARD PASS:
   1. Gradients flow from output to input
   2. LSTM gates updated via Backpropagation Through Time (BPTT)
   3. Encoder and decoder weights updated jointly
   4. Dropout and batch norm affect gradient flow

ğŸ¯ TEACHER FORCING:
   - During training: Decoder sees ground truth previous tokens
   - During inference: Decoder sees its own previous predictions
   - Accelerates training but can cause exposure bias

ğŸ›¡ï¸  REGULARIZATION TECHNIQUES:
   â”œâ”€ Dropout: Prevents co-adaptation of neurons
   â”œâ”€ Batch Normalization: Reduces internal covariate shift
   â”œâ”€ Early Stopping: Prevents overfitting
   â””â”€ Learning Rate Reduction: Adaptive learning

ğŸ’¾ MODEL PARAMETERS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Total Parameters: ~525,000
â”œâ”€ Encoder LSTM: ~263,168 parameters
â”œâ”€ Decoder LSTM: ~263,168 parameters  
â”œâ”€ Dense Output: ~13,107 parameters
â”œâ”€ Batch Norm: ~1,024 parameters
â””â”€ Bias terms: Various

ğŸš€ COMPUTATIONAL COMPLEXITY:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Time Complexity: O(sequence_length Ã— hidden_unitsÂ²)
Space Complexity: O(batch_size Ã— sequence_length Ã— hidden_units)
FLOPs per forward pass: ~50M operations
GPU Memory Usage: ~2-4GB (depending on batch size)

ğŸ¯ SEQUENCE PROCESSING EXAMPLE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Input:  [1, 2, 3, 4, 5, 6] â†’ Encoder â†’ Hidden States
States: [h, c] â†’ Decoder â† Target: [4, 5, 6]
Output: [5, 6, 7] (predicted continuation)

The model learns to continue integer sequences by understanding the pattern
that each output element should be the input element + 1.
"""

def generate_data_flow_diagram():
    """Generate a data flow diagram showing tensor shapes through the network."""
    return """
ğŸ“Š TENSOR FLOW DIAGRAM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                           ğŸ² RAW DATA GENERATION
                                      â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Random Integer Sequences  â”‚
                        â”‚ [1,2,3,4,5,6] â†’ [4,5,6,7] â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                           ğŸ”„ ONE-HOT ENCODING
                                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                        PREPROCESSING                              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Input Sequences:    [15000 Ã— 6]     â†’ [15000 Ã— 6 Ã— 51]           â”‚
    â”‚  Target Input:       [15000 Ã— 3]     â†’ [15000 Ã— 3 Ã— 51]           â”‚
    â”‚  Target Output:      [15000 Ã— 3]     â†’ [15000 Ã— 3 Ã— 51]           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                              ğŸ“¦ BATCH CREATION
                                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                        BATCH TENSORS                              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  X1 (Encoder):       [64 Ã— 6 Ã— 51]                                â”‚
    â”‚  X2 (Decoder):       [64 Ã— 3 Ã— 51]                                â”‚
    â”‚  y (Target):         [64 Ã— 3 Ã— 51]                                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                        ENCODER                                  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Input:              [64 Ã— 6 Ã— 51]                              â”‚
    â”‚         â”‚                                                       â”‚
    â”‚         â–¼                                                       â”‚
    â”‚  LSTM:               [64 Ã— 256] (final hidden state)            â”‚
    â”‚         â”‚                                                       â”‚
    â”‚         â–¼                                                       â”‚
    â”‚  Dropout:            [64 Ã— 256] (20% dropped)                   â”‚
    â”‚         â”‚                                                       â”‚
    â”‚         â–¼                                                       â”‚
    â”‚  BatchNorm:          [64 Ã— 256] (normalized)                    â”‚
    â”‚         â”‚                                                       â”‚
    â”‚         â–¼                                                       â”‚
    â”‚  States:             h=[64 Ã— 256], c=[64 Ã— 256]                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                              ğŸ”„ STATE TRANSFER
                                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                        DECODER                                    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Input:              [64 Ã— 3 Ã— 51]                                â”‚
    â”‚  Initial States:     h=[64 Ã— 256], c=[64 Ã— 256]                   â”‚
    â”‚         â”‚                                                         â”‚
    â”‚         â–¼                                                         â”‚
    â”‚  LSTM:               [64 Ã— 3 Ã— 256] (all timesteps)               â”‚
    â”‚         â”‚                                                         â”‚
    â”‚         â–¼                                                         â”‚
    â”‚  Dropout:            [64 Ã— 3 Ã— 256] (20% dropped)                 â”‚
    â”‚         â”‚                                                         â”‚
    â”‚         â–¼                                                         â”‚
    â”‚  BatchNorm:          [64 Ã— 3 Ã— 256] (normalized)                  â”‚
    â”‚         â”‚                                                         â”‚
    â”‚         â–¼                                                         â”‚
    â”‚  Dense:              [64 Ã— 3 Ã— 51] (vocabulary logits)            â”‚
    â”‚         â”‚                                                         â”‚
    â”‚         â–¼                                                         â”‚
    â”‚  Softmax:            [64 Ã— 3 Ã— 51] (probabilities)                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                              ğŸ“Š LOSS COMPUTATION
                                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    CATEGORICAL CROSSENTROPY                       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Predictions:        [64 Ã— 3 Ã— 51] (softmax output)               â”‚
    â”‚  True Labels:        [64 Ã— 3 Ã— 51] (one-hot targets)              â”‚
    â”‚         â”‚                                                         â”‚
    â”‚         â–¼                                                         â”‚
    â”‚  Loss per sample:    [64 Ã— 3] (per timestep loss)                 â”‚
    â”‚         â”‚                                                         â”‚
    â”‚         â–¼                                                         â”‚
    â”‚  Batch Loss:         scalar (averaged across batch)               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                              â¬…ï¸ BACKPROPAGATION
                                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                      GRADIENT COMPUTATION                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Gradients flow backward through:                                 â”‚
    â”‚  1. Softmax â†’ Dense layer                                         â”‚
    â”‚  2. Dense â†’ Batch Norm â†’ Dropout                                  â”‚
    â”‚  3. Decoder LSTM (through time)                                   â”‚
    â”‚  4. State connections                                             â”‚
    â”‚  5. Encoder LSTM (through time)                                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                              âš™ï¸ PARAMETER UPDATE
                                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                        ADAM OPTIMIZER                             â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Learning Rate:      0.001                                        â”‚
    â”‚  Momentum Terms:     Î²â‚=0.9, Î²â‚‚=0.999                             â”‚
    â”‚  Parameters Updated: ~525,000 weights and biases                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ” TENSOR SHAPE EVOLUTION SUMMARY:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Raw Data:     [sequence_length] integers
One-Hot:      [sequence_length, vocabulary_size] 
Batched:      [batch_size, sequence_length, vocabulary_size]
Encoder:      [batch_size, sequence_length, features] â†’ [batch_size, units]
Decoder:      [batch_size, out_seq_len, features] â†’ [batch_size, out_seq_len, vocab]
Output:       [batch_size, output_sequence_length, vocabulary_size]
"""

def save_diagrams_to_file(content, filename=None):
    """Save the diagrams to a text file."""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"neural_network_architecture_{timestamp}.txt"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("ENCODER-DECODER NEURAL NETWORK ARCHITECTURE DOCUMENTATION\n")
        f.write("=" * 80 + "\n")
        f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")
        f.write(content)
    print(f"ğŸ“„ Architecture diagrams saved to: {filename}")
    return filename

def main():
    parser = argparse.ArgumentParser(description="Visualize neural network architecture")
    parser.add_argument("--detailed", action="store_true", 
                       help="Show detailed technical architecture")
    parser.add_argument("--dataflow", action="store_true",
                       help="Show tensor data flow diagram")
    parser.add_argument("--save", action="store_true",
                       help="Save diagrams to file")
    parser.add_argument("--all", action="store_true",
                       help="Show all diagrams")
    args = parser.parse_args()
    content = ""
    if args.all or (not args.detailed and not args.dataflow):
        print("ğŸ§  BASIC NEURAL NETWORK FLOWCHART")
        print("=" * 80)
        # Import and show the basic flowchart from main (handle import safely).
        try:
            from main import display_neural_network_flowchart
            display_neural_network_flowchart()
        except ImportError:
            print("âš ï¸  Could not import basic flowchart from main.py")
            print("   This is expected during testing.")
        content += "BASIC FLOWCHART\n" + "=" * 40 + "\n"
    if args.detailed or args.all:
        detailed_diagram = generate_detailed_architecture_diagram()
        print(detailed_diagram)
        content += detailed_diagram + "\n\n"
    if args.dataflow or args.all:
        dataflow_diagram = generate_data_flow_diagram()
        print(dataflow_diagram)
        content += dataflow_diagram + "\n\n"
    if args.save and content:
        save_diagrams_to_file(content)

# The big red activation button.
if __name__ == "__main__":
    main()
